\chapter{Introduction}
% \begin{Definition}
%     PoliMi: Politecnico di Milano
% \end{Definition}

%% \smallskip introduces a small vertical skip
%% \bigskip introduces a big vertical skip

%============================= INTRODUCTION =================================
{
    \setlength\epigraphwidth{10cm}
    \setlength\epigraphrule{0pt}
    \renewcommand{\epigraphflush}{center}
    \epigraph{
        I am convinced that machines can and will think. I don't mean that
        machines will behave like men. I don't think for a very long time we
        are going to have a difficult problem distinguishing a man from a
        robot. And I don't think my daughter will ever marry a computer. But I
        think that computers will be doing the things that men do when we say
        they are thinking. I am convinced that machines can and will think in
        our lifetime.}{-- \textup{The Thinking Machine (Artificial Intelligence
            in the 1960s)}, \\ O. Selfridge (Lincoln labs, MIT)}
}

The dream of machines that can think and act as substitutes for humans in their
jobs dates back to the '60s, if not before. We are still not at that point,
although in the last decade the field experienced outstanding advancements and
has been the object of increasing interest.

Artificial intelligence (AI) is a broad field that aims to develop intelligent
software that can for instance acquire knowledge from its interaction with the
world, find optimized strategies for problem solving, automate tasks, detect
patterns in audio, video and textual data, play games, drive cars and much
more. Machine Learning (ML), a subset of artificial intelligence, set the
state-of-art in many fields, such as e.g., image classification~\cite{
Krizhevsky-2012,szegedy2016inception,visin2015renet}, semantic
segmentation~\cite{chen2015semantic, Visin_2016_CVPR_Workshops}, video
understanding~\citep{srivastava2015unsupervised, Xu-et-al-arxiv2015}, natural
language processing and machine translation~\citep{Bahdanau-et-al-arxiv2014}.
Much of this research is already in commercial products we use every day, such
as e.g., speech recognition and speech synthesis in phones, face detection in
cameras and social networks, or traffic signs enhancement in cars. Even more
impressively, a ML algorithm recently won several games of go~\citep{
silver2016alphago} -- a game known for being extremely challenging -- against
one of the best human players.

Despite its many successes, machine learning is no lamp genie that can tackle
any problem by simply providing it with enough data. To get results in machine
learning requires a meticulous analysis of the characteristics of the problem,
clever architecture modelling, smart engineering, as well as careful inspection
of complex and extremely nonlinear compositions of transformations. Most of
all, it requires good organization, intuition and patience, since many of the
experiments can last days if not weeks -- even on big clusters of GPUs.

Many areas of research could benefit from the application of machine learning
techniques, I chose to focus my research on visual scene understanding. My
claim is that understanding a visual scene -- be it an image or a video --
requires capturing the scene semantics, and that this has to be done by
building an incremental representation of the context while processing the
elements of the observed environment scene. For this reason I decided to focus
on Recurrent Neural Networks (RNNs), a family of neural networks with memory --
or state -- that can decide autonomously when to store, retrieve or delete
information from their memory.

As a first step to address the problem of image understanding, I focused on
object classification, i.e., the problem of selecting the class an object in a
scene belongs to. Historically, this problem was addressed by hand-engineering
global and local descriptors as characteristic as possible, so that their
presence or absence could be used as a proxy for the presence or absence of a
specific class of objects. From 2012 onwards, handcrafted methods were
abandoned in favour of convolutional neural networks (CNNs), after the
CNN-based model presented in~\cite{Krizhevsky-2012} improved the state of the
art by $10\%$. Since then, CNNs-based models dominated the object
classification panorama.

In~\cite{visin2015renet} my co-authors and I presented ReNet, an alternative to
the ubiquitous CNNs for object classification. Our model is based on 4 RNNs
that scan the image in 4 directions. RNNs have the potential to store in their
memory any information that is relevant to retain the context of the part of
the image they have seen up to that moment. The first two RNNs scan each line
of the image reading one pixel (or patch, depending on the configuration) at
the time from left to right and from right to left, respectively. The two
resulting feature maps (i.e., output of each of these RNNs) are concatenated in
each position over the channel axis, yielding a composite feature map where
each position has information on the context of the full row as, in each
position, it is a concatenation of an RNN reaching the position from the right
and of an RNN reaching the same position from the left.

The second two RNNs sweep over the composite feature map vertically, top-down
and bottom-up respectively. By reading the composite feature map, each RNN has
access in each position to a "summary" of the corresponding row. Once again,
the two feature maps are concatenated, resulting in a final feature map where
each position is specific to a pixel (or patch) of the image but has
information on the full image. The ReNet architecture allow us to capture the
full context of the image with just one layer (to be fair, two sublayers), as
opposed to CNN based architectures that would need many layers to span the
entire image. As usual, it is still possible to stack multiple ReNet layers to
increase the capacity of the network. ReNet obtained comparable results to the
CNN state-of-the-art on three widely used datasets.

Encouraged by the results of ReNet and the positive feedback from the
scientific community, I worked on a second model based on ReNet, to perform
fine-grained Object Segmentation (i.e., to classify each pixel of the image as
belonging to a specific class). Being able to classify objects without losing
information on their position in the image can be exploited to allow very
precise pixel-level Object Localization, which is essential to many
applications and to a proper understanding of the image.

ReSeg~\citep{Visin_2016_CVPR_Workshops} takes advantage of the inner structure
of the ReNet layers that, in contrast to classical convolutional models, allows
the propagation of information through several layers of computation, retaining
the topological structure of the input. To speed up training, the image is
first preprocessed with a CNN pretrained on big datasets for object
classification, to extract meaningful features and exploit the extra training
data. Those rich features are then processed by several ReNet layers. However
this results in an intermediate feature map that has a smaller resolution
than the image. To be able to classify each pixel, the original resolution has
to be recovered. Thus, one or more transposed convolutional layers~\citep{
dumoulin2016guide} upsample the feature map to the desired size. ReSeg obtained
state-of-the-art on three datasets and won the best paper award at the
DeepVision Workshop at CVPR 2016.

The natural next step in the direction of visual scene understanding is the
processing of videos to exploit the temporal correlation between frames and
improve the performance of the algorithm. It is not trivial to work with videos
in the domain of semantic segmentation: big enough dataset are still lacking
due to the very high cost of labelling each pixel of each frame of a video; in
many cases labels are imprecise and noisy, or missing a well defined semantic
(e.g. the classes "porous" and "vertical mix" are difficult to define
unequivocally), which makes learning harder. Still, it is a challenging but
important problem to tackle and there seems to be room for improvement w.r.t.
the current state of the art on the path traced by ReSeg. The proposed model
combines the benefits of CNNs -- namely the exploitation of the topological
structure in the images and the processing speed -- and the ability to retain
temporal and spatial context information of RNNs. The idea builds on~\cite{
xingjian2015convolutional} that introduced an RNN model whose internal state is
convolutional. We improved on the original model by stacking several
convolutions inside the RNN state (as opposed to only one) and by introducing a
\emph{deconvolutional RNN}, whose state is a stack of multiple transposed
convolutions. This model achieved so far comparable results with the
state-of-the-art on one datasets and encouraging results on two others.

% As future steps for this research, I plan to conclude the video segmentation
% paper in time to submit it to the CVPR conference 2017. I am also working on
% a follow-up of the ReSeg paper with more experiments and a detailed description
% of the model, to be published as a book chapter in an upcoming book on the
% DeepVision workshop. When this part of the research will be over, we plan
% to focus on two main topics of research: applications of ReSeg-like
% architectures to 3D data to predict depth from images coming from a single
% camera, as well as semantic segmentation in the medical domain. We are already
% in touch with a company working in the healthcare field (Imagia) that should
% provide us with suitable datasets to perform bowel polyps detection and
% classification and potentially commercialize our model.

\section{Main contributions}
The main contribution of my research have been the analysis of RNN-based
models in the context of visual data. The ReNet model proved to be an effective
alternative to the ubiquitous CNNs for object classification and draw the
attention of the research community. The subsequent ReSeg model tackled with
success the much harder problem of semantic segmentation, where a fine-grained
understanding of the image structure has to be gained. This paper was selected
at the CVPR 2016 DeepVision Workshop for the best paper award and will be
expanded with a more in-depth analysis of the properties of the architecture in
a chapter of the upcoming book on the CVPR 2016 DeepVision Workshop. Finally I
moved to the analysis of videos for dense segmentation, a very challenging task
characterized by a high computational burden and scarce amounts of labeled
data. To address this task I proposed a model that merges direct convolutions,
transposed convolutions and RNNs in a unique coherent structure. The DEConvLSTM
model proved to be a valid architecture for video semantic segmentation,
pairing the state of the art on one of the historically most used datasets on
this task and achieving encouraging results on two other dataset with just
cursory exploration.


\section{Outline}
The rest of this manuscript is organized as follows:

\begin{itemize}
    \item \autoref{sec:background}: introduces the most important models and
        concepts needed to understand the work done;
    \item \autoref{sec:renet} introduces the problem of object classification
        and describes in detail the ReNet model and its results;
    \item \autoref{sec:reseg} defines what is referred to as semantic
        segmentation and how ReSeg tackles that problem;
    \item \autoref{sec:video_segmentation} moves to video understanding and
        specifically the task of video semantic segmentation and highlights the
        advantages of convolutional-deconvolutional RNNs in this context;
    \item \autoref{sec:conclusion} summarizes the main contribution of this
        research and proposes some of the many possible future directions of
        research that can build on top of this work.
\end{itemize}
