\chapter{Convolutional RNNs for Video Semantic Segmentation}\label{sec:video_segmentation}

The previous chapter introduced the task of semantic segmentation, where the
model is requested to produce a semantic mask, i.e. to classify every pixel of
an input image.

Rather than presenting the algorithm with a single image, a variant of the
segmentation task called cosegmentation, provides it with multiple images
usually taken at different locations in the same scene (potentially by
different cameras). In this setting the model can exploit the information that
the images potentially share to improve the single image prediction and
consequently improve the global score at the same time.

In many ways, \emph{video segmentation} can be seen as an extreme version of
cosegmentation, where the model is asked to segment multiple frames of a video.
As opposed to cosegmentation, where the images depict the same scene from
different angles and in moments potentially very far away in time, in videos the
images come in a seamless fashion and their correlation through time can be
exploited to increase the performances.

The main problem of this task is that if labeling images for semantic
segmentation is expensive, the time required to label all the frames of a
video is even more dramatic. One typical way to alleviate this issue is to
label only a subset of the frames, either by dropping most of the frames and
providing only a few of them with their corresponding mask, or by providing all
the input frames with only a subset of them annotated. Another way to reduce
the labeling effort is to just label the main subject of each frame or to
reduce the number of classes (to the limit case of foreground/background
separation).

Applying machine learning techniques to this class of problems is challenging
due to the lack of large amounts of labeled data, as well as for the huge
amount of time required to train these kind of models that calls for
non-trivial technological solutions. The time spent on operations that are not
strictly related to training, such as loading and preprocessing the data,
saving the weights on disk, saving samples and generating plots to monitor the
progress of the algorithm, has to be minimized. Another recent trend in this
field is to resort to multi-GPU training, that introduces an almost linear
speedup (up to a certain numbers of GPUs)~\cite{theano2016short,ma2016theano}
but increases the complexity of the algorithm, is often subject to constraints
(e.g., the GPUs might have to be hosted by the same machine or node of a
cluster) and makes debugging much less pleasant in many cases.

\autoref{sec:reseg} introduced ReSeg~\citep{Visin_2016_CVPR_Workshops}, an
RNN-based model for image semantic segmentation. One of the main strengths of
ReSeg comes from the coupling of fast general-purpose CNN feature extractors
with stateful RNNs able to carry the information extracted by the CNNs through
various steps of computation, showing the effectiveness of RNNs applied to the
spatial domain.

% While in ReSeg RNNs are allowed to move over the spatial dimension of the data
% rather than the temporal one.
This approach can be taken a step further by allowing the RNNs to jointly
process space and time information in videos, by placing the CNNs directly into
the inner computation of the recursive layer. This chapter introduces a novel
architecture that aims to address the task separation traditionally enforced by
the CNN-RNN dichotomy jointly processing the temporal and the spatial
information at each level of the recurrent-convolutional hierarchy.


\section{Motivation}

% Intro
In the last years, Convolutional Neural Networks (CNNs) have been successfully
applied to address many computer vision tasks, such as image
classification~\citep{Krizhevsky2012-alexnet,Simonyan2015,
Szegedy-et-al-arxiv2014} and object detection~\citep{Girshick-et-al-arxiv2013,
Sermanet13overfeat} and their application has become ubiquitous in many well
known and widely used commercial products.

More recently, pixel-wise prediction of still images has enjoyed the attention
of the computer vision community. In contrast to image classification and
object detection this \emph{structured prediction} problem requires each
pixel to be classified, which demands for a more fine detail understanding of
the image, as well as consistency in the prediction. In this domain many
state of the art architectures successfully coupled classification models
trained on ImageNet~\citep{Simonyan14vgg,Szegedy15googlelenet} with various
upsampling strategies in a trainable end-to-end fashion to address the
challenging image segmentation and scene parsing task~\citep[see e.g.,~][]{
long2014fully,noh2015learning}.

The success witnessed on still images has rapidly reached the video domain,
achieving remarkable results in tasks such as video action recognition~\citep{
simonyan2014two,karpathy2014large}, event detection in videos~\citep{
yeung2015end} and video captioning~\citep{yao2015describing}. Along this
direction, some effort has been devoted lately to address the more
challenging task of end to end video semantic segmentation~\citep{Tran16v2v}.

While many computer vision video-related tasks involve predicting only a single
or a few outputs per video, some of them -- such as video semantic
segmentation, change detection and object tracking -- require a per
\emph{temporal "voxel"} prediction. As an analogy to 3D voxels, where a
volumetric pixel (or voxel) is used to describe a local portion of the data
that extends across space in three dimensions, temporal voxels refer to 3D
portions of the data that span over a contiguous two-dimensional space and over
a one-dimensional sequence in the temporal domain.
Works in this direction usually define a fixed {\emph a priori} window on the
temporal dimension to obtain a voxel-wise prediction~\citep{Tran16v2v}.

The most common way to address video related tasks with neural network
approaches is to combine CNNs with RNNs in a sequential fashion, by first
applying a (possibly pre-trained) CNN on each frame and then interpreting the
resulting feature maps in a temporal consistent way by processing them with an
RNN~\cite{Donahue-et-al-arxiv2014,Vinyals-et-al-CVPR2015,Karpathy+Li-CVPR2015,
Venugopalan_2015_ICCV}. While this is an effective way to exploit the temporal
information, the two pipelines don't completely benefit from each other, since
the temporal consistency is enforced only in the last step. Furthermore, the
spatial relations are mostly ignored by the state of the RNN as they are taken
into account only through the encoding coming from the CNNs.

On this note, \cite{ShiCWYWW15} propose an interesting approach that deeply
entangles the convolution operator with the state-to-state and input-to-state
transitions of the RNN itself, rather than encoding each frame independently
with CNNs and then processing the spatial encodings timewise with an RNN.

The model presented in this chapter, called DeconvRNN, builds on top
of~\cite{ShiCWYWW15} and extends their work in two ways: i) replacing the
convolution operator in the state-to-state and input-to-state transitions with
a full multi-layer convolutional subnetwork ii) introducing a novel
recurrent-convolutional-upsampling layer that learns how to leverage the
spatio-temporal dimensions jointly to produce an upsampled feature map that
respects the 2D topology of the input feature map and at the same time
maintains its time consistency.

The full model mimics the conventional 2D semantic segmentation encoder-decoder
architecture, but is able to jointly capture spatio-temporal information, using
the internal memory of RNNs instead of a fixed window of time.

% We demonstrate the effectiveness of recurrent-convolutional LSTM for video
% semantic segmentation task.
% We provide and in-depth analysis on the importance of the temporal dimension
% for this kind of task.
% - how temporal dimension is important for the task


\section{Model description}\label{sec:deconvrnn_model}

The architecture of the model, shown in~\autoref{fig:deconvrnn_model}, takes
inspiration from the convolutional LSTM (ConvLSTM) proposed in~\cite{
ShiCWYWW15}. This section will first analyze the ConvLSTM model and then
gradually introduce the novelties of DeconvRNN.

\begin{figure}[t]
    \centering
    \includegraphics[width=7cm]{img/deconvRNN/method.png}
    \includegraphics[width=7cm]{img/deconvRNN/model_skip.png}
    \caption{An example of architecture for video semantic segmentation build
        with four convolutional LSTMs. The input to the model is a sequence of
        video frames and the output is a sequence of segmentation maps. The
        arrows indicate if a layer performs spatial down-sampling
        ($\downarrow$) or up-sampling ($\uparrow$). The rectangles show the
        spatial dimension of consecutive hidden states (black rectangles) and
        cell states (green rectangles). In our architecture we use
        bidirectional LSTM which are shown with ($\leftrightarrow$) and skip
        connections.}\label{fig:deconvrnn_model}
\end{figure}


\subsection{Convolutional LSTM}

In~\cite{ShiCWYWW15} the authors propose a modification of LSTM with
peephole, called convolutional LSTM. To allow for a more convenient comparison,
the equations that govern a vanilla LSTM with peephole connections that were
defined in~\autoref{eq:LSTM_peepholes} are reported again here

\begin{equation*}\tag{\ref{eq:LSTM_peepholes} revisited}
\begin{split}
    i_t &= \sigma\left(W_i \cdot \left[h_{t-1}, x_t\right] +
        W_{ic} \hadamard C_{t-1} + b_i \right)\\
    f_t &= \sigma\left(W_f \cdot \left[h_{t-1}, x_t\right] +
        W_{fc} \hadamard C_{t-1} + b_f \right)\\
    \tilde C_t &= tanh \left(W_c \cdot \left[h_{t-1}, x_t\right] + b_c \right)\\
    C_t &= f_t \hadamard C_{t-1} + i_t \hadamard \tilde C_t\\
    o_t &= \sigma\left(W_o \cdot \left[h_{t-1}, x_t\right] +
        W_{oc} \hadamard C_{t-1} + b_o \right)\\
    h_t &= o_t \hadamard tanh \left(C_t\right).
\end{split}
\end{equation*}

\noindent where $x_t$ and $h_t$ are the input and output feature maps
respectively at time $t$; $C_t$ is the state of the cell (or memory) of the
recurrent unit at time $t$; $i_t$, $f_t$, $o_t$ are respectively the states of
the input gate, the forget gate and the output gate and control the behavior of
the recurrent unit given the input and its state at each time step $t$;
$\sigma$ is the logistic function, $W_{\star}$ the weights
and $\hadamard$ the Hadamard product. Note that the compact notation
$W_\star * [h_{t-1}, x_t]$ represents $W_{\star h} * h_{t-1} + W_{\star x} *
x_t$.

The convolutional LSTM of~\cite{ShiCWYWW15} replaces the dot products with
convolutions, leading to the following formulation

\begin{equation}
\begin{split}
    i_t &= \sigma\left(W_i * \left[h_{t-1}, x_t\right] +
        W_{ic} \hadamard C_{t-1} + b_i \right)\\
    f_t &= \sigma\left(W_f * \left[h_{t-1}, x_t\right] +
        W_{fc} \hadamard C_{t-1} + b_f \right)\\
    \tilde C_t &= tanh \left(W_c * \left[h_{t-1}, x_t\right] + b_c \right)\\
    C_t &= f_t \hadamard C_{t-1} + i_t \hadamard \tilde C_t\\
    o_t &= \sigma\left(W_o * \left[h_{t-1}, x_t\right] +
        W_{oc} \hadamard C_{t-1} + b_o \right)\\
    h_t &= o_t \hadamard tanh \left(C_t\right).
\end{split}
\end{equation}

\noindent where $*$ is the convolution operator. Notably, as opposed to
classical recurrent architectures, all gates and states here have \emph{spatial
dimensions}, as they are the result of the application of convolutions on the
input of the layer.

The encoding pathway of the DeconvRNN model builds from this, with two
modifications: i) it introduces a full, multi-layered, CNN, in place of the
convolution; ii) it replaces the Hadamard product with a convolution. The
latter modification allows the network to process videos with different
resolution than the ones it was trained on. To shorten the notation, once again
the state weight matrices $W_{\star c}$ are incorporated into the set of
weights $W$. This results in the following equations

\begin{equation}
\begin{split}
    i_t &= \sigma\left(W_i^{(L)} * \dots * \gamma W_i^{(1)} *
        \gamma W_i^{(0)} * \left[h_{t-1}, x_t, C_{t-1}\right] + b_i \right)\\
    f_t &= \sigma\left(W_f^{(L)} * \dots * \gamma W_f^{(1)} *
        \gamma W_f^{(0)} * \left[h_{t-1}, x_t, C_{t-1}\right] + b_f \right)\\
    \tilde C_t &= tanh \left(W_c^{(L)} * \dots * \gamma W_c^{(1)} *
        \gamma W_c^{(0)} * \left[h_{t-1}, x_t\right] + b_c \right)\\
    C_t &= f_t \hadamard C_{t-1} + i_t \hadamard \tilde C_t\\
    o_t &= \sigma\left(W_o^{(L)} * \dots * \gamma W_o^{(1)} *
        \gamma W_o^{(0)} * \left[h_{t-1}, x_t, C_{t-1}\right] + b_o \right)\\
    h_t &= o_t \hadamard tanh \left(C_t\right).
\end{split}
\end{equation}

\subsection{DeConvolutional LSTM}

\subsection{Spatial resolution}
In order to mimic CNN, we introduce mechanisms to change spatial resolution of
feature maps: e.g., pooling, strided convolution, unpooling or transposed
convolution.

NOTE: What is the difference between putting the pooling in the middle of two
ConvLSTMs and within ConvLSTM??? It might be difficult to defend this as
novelty...

\subsection{Bidirectional RNNs}
Bidirectional RNNs (B-RNNs) \cite{Schuster1997bidirecrnn} have been
successfully used in the literature for video related tasks \cite{Du2015_CVPR}.
BRNNs integrate information from both future and past frames by going through
the video frames both forwards and backwards, ensuring more consistent temporal
smoothness when making predictions. The output of the respective forward and
backward passes is then combined via concatenation or sum and used as input to
the next layer of the architecture or used as final prediction.

Due to memory constraints, we don't follow the trend of concatenating the B-RNN
outputs \cite{}, but rather sum them as initially suggested in
\cite{Schuster1997bidirecrnn}. However, instead of computing the arithmetic
mean, we learn the parameters of a weighted sum

\[ H_j^{l\leftrightarrow} = \alpha_j^{l} H_j^{l\leftarrow} + (1 - \alpha_j^{l}) H_j^{l\rightarrow}\]

\subsection{Skip Connections}


\section{Results}\label{sec:deconvRNN_results}

it has affine transformation, flipping, cropping, warping, etc...
%have 80% + of accuracy in val in gatech
% 46 epochs, val loss 0.4873, val jacquard 0.4865...
% Epoch 53/300 - loss: 0.2103 - cat_masked_accuracy: 0.9281 - val_loss: 0.3016 - val_jaccard: 0.6224
FCN8, FCN8 with convLSTM, our model
\subsection{FCN8}
FCN8 network achieved state of the art results on semantic segmentation tasks
\cite{DBLP:conf/cvpr/LongSD15}. The FCN8 network is divided into two parts : a
contracting part and a decontracting part, so that the output of the network
has the same size than the input. The contracting part is adapted from vgg
\cite{Simonyan14vgg}. The decontracting part is made of 3 transposed
convolutions, to what paper meaning, add citation, which output a slighty
larger map than the input. A crop layer then reshape the output to the exact
same size of the input. Two skip connections are added between layers in the
contracting path and the corresponding one in decontracting path.

We reused the weights of the contrating path without retraining them. However,
we retrain all the decontracting path when used.

This FCN8 network is used a baseline in 3 configurations :
\begin{itemize}
    \item As is. No modification to the network is done. Only the decontracting
        path is trained. The network has no temporality taken into account.
    \item We add a convolutional LSTM layer on top of the network. The time is
        taken into account only at the top scale.
    \item The transposed convolutions of the decontracting path are replaced by
        upsampling LSTM layers. The temporality is taken into account at
        different scales in the decontracting path. Like in the previous point
        convolutional LSTM layer is added on top. The skip connections of the
        original FCN8 are removed.
\end{itemize}


\subsection{Datasets}

\subsubsection{CamVid: Motion-based Video Segmentation}
The Cambridge-driving Labeled Video Database (CamVid)~\cite{
Brostow2010semantic}~\footnote{http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/}
is a motion-based video segmentation dataset, captured
from the perspective of a driving automobile, with object class semantic
labels. The dataset contains, which contains 11 semantic classes such as sky,
road, buildings or cars. The dataset contains video sequences but has more
often been used for still image semantic segmentation, considering the video
frames separately \cite{}.

\subsubsection{Gatech: Geometric Context Video Segmentation}
\label{sssec:gatech}
Gatech dataset
\footnote{http://www.cc.gatech.edu/cpl/projects/videogeometriccontext/}
\cite{VideoGeometricContext2013} is a geometric scene understanding dataset,
which consists of 63 videos for training/validation and 38 for testing. Each
video has between 50 and 300 frames (with an average of 190). A pixel-wise
segmentation map is provided for each frame. There are 8 classes in the
dataset: \textit{sky}, \textit{ground}, \textit{buildings}, \textit{porous},
\textit{humans}, \textit{cars}, \textit{vertical mix} and \textit{main mix}.
The dataset was originally built to learn 3D geometric structure of outdoor
video scenes.


\subsubsection{Change Detection}
\label{sssec:changedet}
Change Detection dataset (CDnet) comes after 2 successful benchmarks
\cite{wang2014cdnet,goyette2012changedetection}. CDnet contains over 160,000
frames obtained from 31 videos and manually annotated. This dataset covers a
variety of semantic segmentation scenarios. The categories include Challenging
weather, Air turbulence, camera jitter, dynamic background, low resolution
traffic videos, intermittent object motion, shadow, thermal, bad weather, low
frame rate, night videos, pan-tilt-zoom and turbulence. The official evaluation
metrics are Recall (Re), Specificity (Sp), False Positive Rate (FPR), False
Negative Rate (FNR), Percentage of Wrong Classifications (PWC), Precision (Pr)
and F-Measure (FM).

\subsection{Experimental Setup}
\label{ssec:setup}
In this section, we study the effectiveness of involving the temporal
information on the task of video spatial segmentaion. Our experiments with
regarding to our pipeline are designed to find an optimum way of involving the
temporal information.
Toward this end, we compare our end to end segmentation method with two
following baselines:
\begin{itemize}
    \item Segmentation over each frame independently using Convnet. In this
        case, we don't involve any temporal information.
    \item Employing the temporal information only on half of the network. (TODO:
        why are we using this as a baseline?)
\end{itemize}
As explained earlier we employ the temporal information on every layers of the
network and have a recursion over time at each layer. Therefore, we have both
convolutional LSTM and upsampling LSTM. (TODO: requires more explanation)
\subsubsection{Configuration of the layers}
The convolutional and upsampling layers respectively consist of 70 and 100
units. We apply batch normalization on each scale.

\subsection{Leveraging time throughout the feature hierarchy}
\label{ssec:analysis}

\section{Discussion}
\label{sec:disc}
\section{Conclusion}
