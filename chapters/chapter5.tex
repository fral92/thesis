\chapter{Convolutional RNNs for Video Semantic Segmentation}\label{sec:video_segmentation}

The previous chapter introduced the task of semantic segmentation, where the
model is requested to produce a semantic mask, i.e. to classify every pixel of
an input image.

Rather than presenting the algorithm with a single image, a variant of the
segmentation task called cosegmentation, provides it with multiple images
usually taken at different locations in the same scene (potentially by
different cameras). In this setting the model can exploit the information that
the images potentially share to improve the single image prediction and
consequently improve the global score at the same time.

In many ways, \emph{video segmentation} can be seen as an extreme version of
cosegmentation, where the model is asked to segment multiple frames of a video.
As opposed to cosegmentation, where the images depict the same scene from
different angles and in moments potentially very far away in time, in videos the
images come in a seamless fashion and their correlation through time can be
exploited to increase the performances.

The main problem of this task is that if labeling images for semantic
segmentation is expensive, the time required to label all the frames of a
video is even more dramatic. One typical way to alleviate this issue is to
label only a subset of the frames, either by dropping most of the frames and
providing only a few of them with their corresponding mask, or by providing all
the input frames with only a subset of them annotated. Another way to reduce
the labeling effort is to just label the main subject of each frame or to
reduce the number of classes (to the limit case of foreground/background
separation).

Applying machine learning techniques to this class of problems is challenging
due to the lack of large amounts of labeled data, as well as for the huge
amount of time required to train these kind of models that calls for
non-trivial technological solutions. The time spent on operations that are not
strictly related to training, such as loading and preprocessing the data,
saving the weights on disk, saving samples and generating plots to monitor the
progress of the algorithm, has to be minimized. Another recent trend in this
field is to resort to multi-GPU training, that introduces an almost linear
speedup (up to a certain numbers of GPUs)~\cite{theano2016short,ma2016theano}
but increases the complexity of the algorithm, is often subject to constraints
(e.g., the GPUs might have to be hosted by the same machine or node of a
cluster) and makes debugging much less pleasant in many cases.

\autoref{sec:reseg} introduced ReSeg~\citep{Visin_2016_CVPR_Workshops}, an
RNN-based model for image semantic segmentation. One of the main strengths of
ReSeg comes from the coupling of fast general-purpose CNN feature extractors
with stateful RNNs able to carry the information extracted by the CNNs through
various steps of computation, showing the effectiveness of RNNs applied to the
spatial domain.

% While in ReSeg RNNs are allowed to move over the spatial dimension of the data
% rather than the temporal one.
This approach can be taken a step further by allowing the RNNs to jointly
process space and time information in videos, by placing the CNNs directly into
the inner computation of the recursive layer. This chapter introduces a novel
architecture that aims to address the task separation traditionally enforced by
the CNN-RNN dichotomy jointly processing the temporal and the spatial
information at each level of the recurrent-convolutional hierarchy.


\section{Motivation}

% Intro
In the last years, Convolutional Neural Networks (CNNs) have been successfully
applied to address many computer vision tasks, such as image
classification~\citep{Krizhevsky2012-alexnet,Simonyan2015,
Szegedy-et-al-arxiv2014} and object detection~\citep{Girshick-et-al-arxiv2013,
Sermanet13overfeat} and their application has become ubiquitous in many well
known and widely used commercial products.

More recently, pixel-wise prediction of still images has enjoyed the attention
of the computer vision community. In contrast to image classification and
object detection this \emph{structured prediction} problem requires each
pixel to be classified, which demands for a more fine detail understanding of
the image, as well as consistency in the prediction. In this domain many
state of the art architectures successfully coupled classification models
trained on ImageNet~\citep{Simonyan14vgg,Szegedy15googlelenet} with various
upsampling strategies in a trainable end-to-end fashion to address the
challenging image segmentation and scene parsing task~\citep[see e.g.,~][]{
long2014fully,noh2015learning}.

The success witnessed on still images has rapidly reached the video domain,
achieving remarkable results in tasks such as video action recognition~\citep{
simonyan2014two,karpathy2014large}, event detection in videos~\citep{
yeung2015end} and video captioning~\citep{yao2015describing}. Along this
direction, some effort has been devoted lately to address the more
challenging task of end to end video semantic segmentation~\citep{Tran16v2v}.

While many computer vision video-related tasks involve predicting only a single
or a few outputs per video, some of them -- such as video semantic
segmentation, change detection and object tracking -- require a per
\emph{temporal "voxel"} prediction. As an analogy to 3D voxels, where a
volumetric pixel (or voxel) is used to describe a local portion of the data
that extends across space in three dimensions, temporal voxels refer to 3D
portions of the data that span over a contiguous two-dimensional space and over
a one-dimensional sequence in the temporal domain.
Works in this direction usually define a fixed {\emph a priori} window on the
temporal dimension to obtain a voxel-wise prediction~\citep{Tran16v2v}.

The most common way to address video related tasks with neural network
approaches is to combine CNNs with RNNs in a sequential fashion, by first
applying a (possibly pre-trained) CNN on each frame and then interpreting the
resulting feature maps in a temporal consistent way by processing them with an
RNN~\cite{Donahue-et-al-arxiv2014,Vinyals-et-al-CVPR2015,Karpathy+Li-CVPR2015,
Venugopalan_2015_ICCV}. While this is an effective way to exploit the temporal
information, the two pipelines don't completely benefit from each other, since
the temporal consistency is enforced only in the last step. Furthermore, the
spatial relations are mostly ignored by the state of the RNN as they are taken
into account only through the encoding coming from the CNNs.

On this note, \cite{ShiCWYWW15} propose an interesting approach that deeply
entangles the convolution operator with the state-to-state and input-to-state
transitions of the RNN itself, rather than encoding each frame independently
with CNNs and then processing the spatial encodings timewise with an RNN.

The model presented in this chapter builds on top of~\cite{ShiCWYWW15} and
extends their work in two ways: i) replacing the convolution operator in the
state-to-state and input-to-state transitions with a full multi-layer
convolutional subnetwork ii) introducing a novel
recurrent-convolutional-upsampling layer that learns how to leverage the
spatio-temporal dimensions jointly to produce an upsampled feature map that
respects the 2D topology of the input feature map and at the same time
maintains its time consistency.

The full model mimics the conventional 2D semantic segmentation encoder-decoder
architecture, but is able to jointly capture spatio-temporal information, using
the internal memory of RNNs instead of a fixed window of time.

% We demonstrate the effectiveness of recurrent-convolutional LSTM for video
% semantic segmentation task.
% We provide and in-depth analysis on the importance of the temporal dimension
% for this kind of task.
% - how temporal dimension is important for the task


\section{Method}

Our architecture is shown in Figure \ref{fig:method}.
\label{sec:method}
\begin{figure}
\centering
\includegraphics[width=7cm]{img/deconvRNN/model_skip.png}
\label{fig:method}
\caption{An example of architecture for video semantic segmentation build with
four convolutional LSTMs. The input to the model is a sequence of video frames
and the output is a sequence of segmentation maps. The arrows indicate if a
layer performs spatial down-sampling ($\downarrow$) or up-sampling
($\uparrow$). The rectangles show the spatial dimension of consecutive hidden
states (black rectangles) and cell states (green rectangles). In our
architecture we use bidirectional LSTM which are shown with ($\leftrightarrow$)
and skip connections.}
\end{figure}


\subsection{Convolutional LSTM}
We build on top of convolutional LSTM (ConvLSTM) proposed in~\cite{ShiCWYWW15}.
TODO(describe better). + brief LSTM background


\[ i_t^{l} = \sigma(W_{xi}^{l}*X_t^{l} + W_{hi}^{l}*H_{t-1}^{l} + W_{ci}^{l} \circ C_{t-1}^{l} + b_i^{l})\]
\[ f_t^{l} = \sigma(W_{xf}^{l}*X_t^{l} + W_{hf}^{l}*H_{t-1}^{l} + W_{cf}^{l} \circ C_{t-1}^{l} + b_f^{l})\]
\[ C_t^{l} = f_t^{l} \circ C_{t-1}^{l} + i \circ tanh(W_{xc}^{l}*X_t^{l} + W_{hc}^{l}*H_{t-1}^{l} + b_c^{l})\]
\[ o_t^{l} = \sigma(W_{xo}^{l}*X_t^{l} + W_{ho}^{l}*H_{t-1}^{l} + W_{co}^{l} \circ C_{t-1}^{l} + b_o^{l})\]
\[ H_t^{l} = o_t^{l} \circ relu(C_t^{l})\]

where \(X_t^{l}\) and \(H_t^{l}\) are the input and output feature maps
respectively, \(C_t^{l}\) is the state of the cell (or memory) of the recurrent
unit, \(i_t^{l}\), \(f_t^{l}\), \(o_t^{l}\) are respectively the input gate,
the forget gate and the output gate and control the behavior of the recurrent
unit given the input and its state at each time step $t$.
\(\sigma\) sigmoid function, \(W_{\star}^{l}\) weights, \(*\) convolution
operation and \(\circ\) Hadamard product.

Note that, differently from typical recurrent architectures, all gates and
states have spatial dimensions as are the result of the application of
convolutions on the input of the layer. Differently from~\cite{ShiCWYWW15}, we
use Rectified Linear Unit (ReLU) on the output instead of the tanh.

\subsection{Spatial resolution}
In order to mimic CNN, we introduce mechanisms to change spatial resolution of
feature maps: e. g. pooling, strided convolution, unpooling or transposed
convolution.

NOTE: What is the difference between putting the pooling in the middle of two
ConvLSTMs and within ConvLSTM??? It might be difficult to defend this as
novelty...

\subsection{Bidirectional RNNs}
Bidirectional RNNs (B-RNNs) \cite{Schuster1997bidirecrnn} have been
successfully used in the literature for video related tasks \cite{Du2015_CVPR}.
BRNNs integrate information from both future and past frames by going through
the video frames both forwards and backwards, ensuring more consistent temporal
smoothness when making predictions. The output of the respective forward and
backward passes is then combined via concatenation or sum and used as input to
the next layer of the architecture or used as final prediction.

Due to memory constraints, we don't follow the trend of concatenating the B-RNN
outputs \cite{}, but rather sum them as initially  suggested in
\cite{Schuster1997bidirecrnn}. However, instead of computing the arithmetic
mean, we learn the parameters of a weighted sum

\[ H_j^{l\leftrightarrow} = \alpha_j^{l} H_j^{l\leftarrow} + (1 - \alpha_j^{l}) H_j^{l\rightarrow}\]

\subsection{Skip Connections}

\section{Results on Benchmark Datasets}
\label{sec:exp}
FCN8, FCN8 with convLSTM, our model
\subsection{FCN8}
FCN8 network achieved state of the art results on semantic segmentation tasks
\cite{DBLP:conf/cvpr/LongSD15}. The FCN8 network is divided into two parts : a
contracting part and a decontracting part, so that the output of the network
has the same size than the input. The contracting part is adapted from vgg
\cite{Simonyan14vgg}. The decontracting part is made of 3 transposed
convolutions, to what paper meaning, add citation, which output a slighty
larger map than the input. A crop layer then reshape the output to the exact
same size of the input. Two skip connections are added between layers in the
contracting path and the corresponding one in decontracting path.

We reused the weights of the contrating path without retraining them. However,
we retrain all the decontracting path when used.

This FCN8 network is used a baseline in 3 configurations :
\begin{itemize}
    \item As is. No modification to the network is done. Only the decontracting
        path is trained. The network has no temporality taken into account.
    \item We add a convolutional LSTM layer on top of the network. The time is
        taken into account only at the top scale.
    \item The transposed convolutions of the decontracting path are replaced by
        upsampling LSTM layers. The temporality is taken into account at
        different scales in the decontracting path. Like in the previous point
        convolutional LSTM layer is added on top. The skip connections of the
        original FCN8 are removed.
\end{itemize}


\subsection{Datasets}

\subsubsection{CamVid: Motion-based Video Segmentation}
\label{sssec:camvid}
CamVid
dataset\footnote{http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/}
\cite{Brostow90camvid} is a motion-based video segmentation dataset, captured
from the perspective of a driving automobile, with object class semantic
labels. The dataset contains, which contains 11 semantic classes such as sky,
road, buildings or cars. The dataset contains video sequences but has more
often been used for still image semantic segmentation, considering the video
frames separately \cite{}.

\subsubsection{Gatech: Geometric Context Video Segmentation}
\label{sssec:gatech}
Gatech dataset
\footnote{http://www.cc.gatech.edu/cpl/projects/videogeometriccontext/}
\cite{VideoGeometricContext2013} is a geometric scene understanding dataset,
which consists of 63 videos for training/validation and 38 for testing. Each
video has between 50 and 300 frames (with an average of 190). A pixel-wise
segmentation map is provided for each frame. There are 8 classes in the
dataset: \textit{sky}, \textit{ground}, \textit{buildings}, \textit{porous},
\textit{humans}, \textit{cars}, \textit{vertical mix} and \textit{main mix}.
The dataset was originally built to learn 3D geometric structure of outdoor
video scenes.


\subsubsection{Change Detection}
\label{sssec:changedet}
Change Detection dataset (CDnet) comes after 2 successful benchmarks
\cite{wang2014cdnet,goyette2012changedetection}. CDnet contains over 160,000
frames obtained from 31 videos and manually annotated. This dataset covers a
variety of semantic segmentation scenarios. The categories include Challenging
weather, Air turbulence, camera jitter, dynamic background, low resolution
traffic videos, intermittent object motion, shadow, thermal, bad weather, low
frame rate, night videos, pan-tilt-zoom and turbulence. The official evaluation
metrics are Recall (Re), Specificity (Sp), False Positive Rate (FPR), False
Negative Rate (FNR), Percentage of Wrong Classifications (PWC), Precision (Pr)
and F-Measure (FM).

\subsection{Experimental Setup}
\label{ssec:setup}
In this section, we study the effectiveness of involving the temporal
information on the task of video spatial segmentaion. Our experiments with
regarding to our pipeline are designed to find an optimum way of involving the
temporal information.
Toward this end, we compare our end to end segmentation method with two
following baselines:
\begin{itemize}
    \item Segmentation over each frame independently using Convnet. In this
        case, we don't involve any temporal information.
    \item Employing the temporal information only on half of the network. (TODO:
        why are we using this as a baseline?)
\end{itemize}
As explained earlier we employ the temporal information on every layers of the
network and have a recursion over time at each layer. Therefore, we have both
convolutional LSTM and upsampling LSTM. (TODO: requires more explanation)
\subsubsection{Configuration of the layers}
The convolutional and upsampling layers respectively consist of 70 and 100
units. We apply batch normalization on each scale.

\subsection{Leveraging time throughout the feature hierarchy}
\label{ssec:analysis}

\section{Discussion}
\label{sec:disc}
\section{Conclusion}
